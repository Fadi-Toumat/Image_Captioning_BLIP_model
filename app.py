# -*- coding: utf-8 -*-
"""image_captioning_app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14CKflhaonl2zD1-3w2fQ51F2PIXurP8S
"""

# Install necessary libraries
!pip install gradio transformers torch numpy Pillow

import gradio as gr
import numpy as np
from PIL import Image
from transformers import AutoProcessor, BlipForConditionalGeneration
import torch

# Load the pretrained processor and model
# Determine if CUDA (GPU) is available, otherwise use CPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}") # For tracking the device being used in the console

processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)

def caption_image(input_image: np.ndarray):
    """
    Generates a descriptive caption for an input image using the BLIP model.

    Args:
        input_image (np.ndarray): A NumPy array representing the input image.
                                This is the format Gradio's gr.Image() provides.

    Returns:
        str: The generated caption or an error message if an issue occurs.
    """
    if input_image is None:
        return "Please upload an image."

    try:
        # Convert numpy array to PIL Image and ensure it's in RGB format
        raw_image = Image.fromarray(input_image).convert('RGB')

        # Process the image and move tensors to the correct device (CPU/GPU)
        inputs = processor(raw_image, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()} # Move all tensors in the dictionary to the device

        # Generate a caption for the image
        # max_new_tokens: Generates up to 50 new tokens for the caption.
        # num_beams: Uses beam search with 4 beams for better quality generation.
        # early_stopping: Stops generation once an end-of-sequence token is generated.
        out = model.generate(**inputs, max_new_tokens=50, num_beams=4, early_stopping=True)

        # Decode the generated tokens back to human-readable text
        caption = processor.decode(out[0], skip_special_tokens=True)

        return caption

    except Exception as e:
        # Catch any exceptions during processing or generation and return an error message
        return f"An error occurred: {e}"

# Set up the Gradio interface
iface = gr.Interface(
    fn=caption_image,
    inputs=gr.Image(label="Upload your image here"), # Added a clear label for the input component
    outputs="text",
    title="Image Captioning with BLIP Model", # Descriptive title for the web app
    description="Upload an image and let the BLIP model generate a descriptive caption for it. This application leverages the BLIP model from Hugging Face Transformers.",
    allow_flagging="auto" # Allows users to flag (report) outputs, which can be useful for feedback
)

# Launch the Gradio app
# In Colab, you almost always want share=True to get a public URL accessible from your browser
# The local URL (127.0.0.1) is generally not directly accessible in Colab's environment.
iface.launch(share=True) # Always use share=True in Google Colab for accessibility
# server_name and server_port are usually not necessary when share=True in Colab

!pip freeze > requirements.txt

